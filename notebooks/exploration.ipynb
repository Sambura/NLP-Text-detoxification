{"cells":[{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Practical machine learning and deep learning. Lab 5\n","## Competition\n","No competition for today\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning a model on a translation task\n","Today we will be finetunning T5(Text-To-Text Transfer Transformer) [model](https://github.com/google-research/t5x) on translation task. For this purpose we will be using [HuggingFace transformers](https://huggingface.co/docs/transformers/index) and [WMT16](https://huggingface.co/datasets/wmt16) dataset. "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"MOsHUjgdIrIW","outputId":"f84a093e-147f-470e-aad9-80fb51193c8e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (2.14.5)\n","Requirement already satisfied: sacrebleu in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (2.3.1)\n","Requirement already satisfied: transformers[sentencepiece] in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (4.34.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (1.24.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (13.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (2.1.1)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (0.17.3)\n","Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (6.0)\n","Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (3.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (2023.10.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (0.4.0)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (4.24.4)\n","Requirement already satisfied: portalocker in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (2.8.2)\n","Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (4.9.3)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: pywin32>=226 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: numpy==1.24.3 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (1.24.3)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# installing huggingface libraries for dataset, models and metrics\n","%pip install datasets transformers[sentencepiece] sacrebleu\n","\n","%pip install numpy==1.24.3"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.766013Z","iopub.status.busy":"2023-09-24T15:10:01.765366Z","iopub.status.idle":"2023-09-24T15:10:01.772400Z","shell.execute_reply":"2023-09-24T15:10:01.771384Z","shell.execute_reply.started":"2023-09-24T15:10:01.765977Z"},"trusted":true},"outputs":[],"source":["# Necessary inputs\n","import warnings\n","\n","from datasets import load_dataset, load_metric\n","import transformers\n","import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["## Selecting the model\n","For the example purpose we select as model checkpoint the smallest transformer in T5 family - `t5_small`. Other pre-trained models can be found [here](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=T5%20comes%20in%20different%20sizes%3A)."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.774778Z","iopub.status.busy":"2023-09-24T15:10:01.773897Z","iopub.status.idle":"2023-09-24T15:10:01.784448Z","shell.execute_reply":"2023-09-24T15:10:01.783216Z","shell.execute_reply.started":"2023-09-24T15:10:01.774744Z"},"trusted":true},"outputs":[],"source":["# selecting model checkpoint\n","model_checkpoint = \"t5-small\""]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.787917Z","iopub.status.busy":"2023-09-24T15:10:01.787594Z","iopub.status.idle":"2023-09-24T15:10:03.219266Z","shell.execute_reply":"2023-09-24T15:10:03.218277Z","shell.execute_reply.started":"2023-09-24T15:10:01.787893Z"},"id":"IreSlFmlIrIm","trusted":true},"outputs":[],"source":["# setting random seed for transformers library\n","transformers.set_seed(42)\n","\n","# Load the WMT16 dataset\n","raw_datasets = load_dataset(\"wmt16\", \"de-en\")\n","\n","# Load the BLUE metric\n","metric = load_metric(\"sacrebleu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset\n","Downloaded from HuggingFace dataset is a `DatasetDict`. It contains keys `[\"train\", \"validation\", \"test\"]` - which represents a dataset splits"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.220970Z","iopub.status.busy":"2023-09-24T15:10:03.220604Z","iopub.status.idle":"2023-09-24T15:10:03.229080Z","shell.execute_reply":"2023-09-24T15:10:03.228070Z","shell.execute_reply.started":"2023-09-24T15:10:03.220925Z"},"id":"GWiVUF0jIrIv","outputId":"35e3ea43-f397-4a54-c90c-f2cf8d36873e","trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['translation'],\n","        num_rows: 4548885\n","    })\n","    validation: Dataset({\n","        features: ['translation'],\n","        num_rows: 2169\n","    })\n","    test: Dataset({\n","        features: ['translation'],\n","        num_rows: 2999\n","    })\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:35:02.763581Z","iopub.status.busy":"2023-09-24T15:35:02.762411Z","iopub.status.idle":"2023-09-24T15:35:02.772979Z","shell.execute_reply":"2023-09-24T15:35:02.771223Z","shell.execute_reply.started":"2023-09-24T15:35:02.763514Z"},"id":"X6HrpprwIrIz","outputId":"d7670bc0-42e4-4c09-8a6a-5c018ded7d95","trusted":true},"outputs":[{"data":{"text/plain":["{'translation': [{'de': 'Wiederaufnahme der Sitzungsperiode',\n","   'en': 'Resumption of the session'},\n","  {'de': 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.',\n","   'en': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'},\n","  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.',\n","   'en': \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"},\n","  {'de': 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.',\n","   'en': 'You have requested a debate on this subject in the course of the next few days, during this part-session.'},\n","  {'de': 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.',\n","   'en': \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"}]}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# samples from train dataset\n","raw_datasets[\"train\"][:5]"]},{"cell_type":"markdown","metadata":{},"source":["## Metric\n","[Sacrebleu](https://huggingface.co/spaces/evaluate-metric/sacrebleu) computes:\n","- `score`: BLEU score\n","- `counts`: list of counts of correct n-grams\n","- `totals`: list of counts of total n-grams\n","- `precisions`: list of precisions\n","- `bp`: Brevity penalty\n","- `sys_len`: cumulative sysem length\n","- `ref_len`: cumulative reference length\n","\n","The main metric is [BLEU score](https://en.wikipedia.org/wiki/BLEU). BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score measures the similarity of the machine-translated text to a set of high quality reference translations.\n","\n","The BLEU metric is calculates using [n-grams](https://en.wikipedia.org/wiki/N-gram)."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:38:54.393583Z","iopub.status.busy":"2023-09-24T15:38:54.393131Z","iopub.status.idle":"2023-09-24T15:38:54.403154Z","shell.execute_reply":"2023-09-24T15:38:54.401952Z","shell.execute_reply.started":"2023-09-24T15:38:54.393549Z"},"id":"5o4rUteaIrI_","outputId":"18038ef5-554c-45c5-e00a-133b02ec10f1","trusted":true},"outputs":[{"data":{"text/plain":["Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n","Produces BLEU scores along with its sufficient statistics\n","from a source against one or more references.\n","\n","Args:\n","    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n","    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n","    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n","        - `'none'`: no smoothing\n","        - `'floor'`: increment zero counts\n","        - `'add-k'`: increment num/denom by k for n>1\n","        - `'exp'`: exponential decay\n","    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n","    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n","        - `'none'`: No tokenization.\n","        - `'zh'`: Chinese tokenization.\n","        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n","        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n","        - `'char'`: Language-agnostic character-level tokenization.\n","        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n","    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n","    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n","    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n","\n","Returns:\n","    'score': BLEU score,\n","    'counts': Counts,\n","    'totals': Totals,\n","    'precisions': Precisions,\n","    'bp': Brevity penalty,\n","    'sys_len': predictions length,\n","    'ref_len': reference length,\n","\n","Examples:\n","\n","    Example 1:\n","        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n","        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        100.0\n","\n","    Example 2:\n","        >>> predictions = [\"hello there general kenobi\",\n","        ...                 \"on our way to ankh morpork\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n","        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n","        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions,\n","        ...                             references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        39.8\n","\"\"\", stored examples: 0)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# how to use sacrebleu and its purpose\n","metric"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:05:06.585010Z","iopub.status.busy":"2023-09-24T16:05:06.584619Z","iopub.status.idle":"2023-09-24T16:05:06.601506Z","shell.execute_reply":"2023-09-24T16:05:06.600239Z","shell.execute_reply.started":"2023-09-24T16:05:06.584982Z"},"id":"6XN1Rq0aIrJC","outputId":"a4405435-a8a9-41ff-9f79-a13077b587c7","trusted":true},"outputs":[{"data":{"text/plain":["{'score': 45.59274666224604,\n"," 'counts': [7, 4, 1, 0],\n"," 'totals': [9, 6, 3, 2],\n"," 'precisions': [77.77777777777777,\n","  66.66666666666667,\n","  33.333333333333336,\n","  25.0],\n"," 'bp': 1.0,\n"," 'sys_len': 9,\n"," 'ref_len': 9}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["fake_preds = [\"hello there\", \"general kenobi\", \"Can I get an A\"]\n","fake_labels = [[\"hello there\"], [\"general kenobi\"], ['Can I get a C']]\n","metric.compute(predictions=fake_preds, references=fake_labels)"]},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["## Preprocessing the data\n","As usual we will need to preprocess data and tokenize it before passing to model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.310034Z","iopub.status.busy":"2023-09-24T15:10:03.309664Z","iopub.status.idle":"2023-09-24T15:10:03.505289Z","shell.execute_reply":"2023-09-24T15:10:03.504208Z","shell.execute_reply.started":"2023-09-24T15:10:03.310001Z"},"id":"eXNLu_-nIrJI","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# we will use autotokenizer for this purpose\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.541473Z","iopub.status.busy":"2023-09-24T15:10:03.540621Z","iopub.status.idle":"2023-09-24T15:10:03.549770Z","shell.execute_reply":"2023-09-24T15:10:03.548874Z","shell.execute_reply.started":"2023-09-24T15:10:03.541440Z"},"id":"vc0BSBLIIrJQ","trusted":true},"outputs":[],"source":["max_input_length = 1024\n","max_target_length = 1024\n","source_lang = \"en\"\n","target_lang = \"de\"\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n","    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:09:35.662567Z","iopub.status.busy":"2023-09-24T16:09:35.662099Z","iopub.status.idle":"2023-09-24T16:09:37.545512Z","shell.execute_reply":"2023-09-24T16:09:37.544516Z","shell.execute_reply.started":"2023-09-24T16:09:35.662533Z"},"id":"DDtsaJeVIrJT","outputId":"aa4734bf-4ef5-4437-9948-2c16363da719","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84417460785d4cf5b9c2a6ceb181a531","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"NameError","evalue":"name 'prefix' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\PMLDL assignment 1 - Text detoxification\\notebooks\\exploration.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cropped_datasets[\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m raw_datasets[\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m cropped_datasets[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m raw_datasets[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenized_datasets \u001b[39m=\u001b[39m cropped_datasets\u001b[39m.\u001b[39mmap(preprocess_function, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tokenized_datasets[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\datasets\\dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 853\u001b[0m     {\n\u001b[0;32m    854\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    855\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    859\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    867\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    871\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    873\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\datasets\\dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[1;32m--> 854\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    855\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    859\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    867\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    871\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    873\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    874\u001b[0m     }\n\u001b[0;32m    875\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3474\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3470\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   3471\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   3472\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3473\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3474\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3475\u001b[0m         batch,\n\u001b[0;32m   3476\u001b[0m         indices,\n\u001b[0;32m   3477\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(shard\u001b[39m.\u001b[39mlist_indexes()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   3478\u001b[0m         offset\u001b[39m=\u001b[39moffset,\n\u001b[0;32m   3479\u001b[0m     )\n\u001b[0;32m   3480\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3481\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3483\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\datasets\\arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[0;32m   3357\u001b[0m     }\n","\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\PMLDL assignment 1 - Text detoxification\\notebooks\\exploration.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     inputs \u001b[39m=\u001b[39m [prefix \u001b[39m+\u001b[39m ex[source_lang] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m examples[\u001b[39m\"\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     targets \u001b[39m=\u001b[39m [ex[target_lang] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m examples[\u001b[39m\"\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model_inputs \u001b[39m=\u001b[39m tokenizer(inputs, max_length\u001b[39m=\u001b[39mmax_input_length, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\PMLDL assignment 1 - Text detoxification\\notebooks\\exploration.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     inputs \u001b[39m=\u001b[39m [prefix \u001b[39m+\u001b[39m ex[source_lang] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m examples[\u001b[39m\"\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     targets \u001b[39m=\u001b[39m [ex[target_lang] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m examples[\u001b[39m\"\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/exploration.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model_inputs \u001b[39m=\u001b[39m tokenizer(inputs, max_length\u001b[39m=\u001b[39mmax_input_length, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[1;31mNameError\u001b[0m: name 'prefix' is not defined"]}],"source":["# for the example purpose we will crop the dataset and select first 5000 for train\n","# and 500 for validation and test\n","cropped_datasets = raw_datasets\n","cropped_datasets['train'] = raw_datasets['train'].select(range(5000))\n","cropped_datasets['validation'] = raw_datasets['validation'].select(range(500))\n","cropped_datasets['test'] = raw_datasets['test'].select(range(500))\n","tokenized_datasets = cropped_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets['train'][0]"]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Fine-tuning the model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.605060Z","iopub.status.busy":"2023-09-24T15:10:03.604730Z","iopub.status.idle":"2023-09-24T15:10:04.671893Z","shell.execute_reply":"2023-09-24T15:10:04.670859Z","shell.execute_reply.started":"2023-09-24T15:10:03.605029Z"},"id":"TlqNaB8jIrJW","outputId":"84916cf3-6e6c-47f3-d081-032ec30a4132","trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","# create a model for the pretrained model\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def process_prompt(model, prompt, tokenizer=tokenizer):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","    outputs = model.generate(input_ids=input_ids)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True, temperature=0))\n","\n","def get_process_prompt(model, tokenizer=tokenizer):\n","    def callback(prompt):\n","        process_prompt(model, prompt, tokenizer)\n","    \n","    return callback"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["ask = get_process_prompt(model)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["         \n"]}],"source":["detox = \"I like that shit.\"\n","\n","ask(\"repharse:\" + detox)"]}],"metadata":{"colab":{"name":"Translation","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
