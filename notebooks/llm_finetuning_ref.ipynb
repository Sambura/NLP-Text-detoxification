{"cells":[{"cell_type":"markdown","metadata":{"id":"rEJBSTyZIrIb"},"source":["# Practical machine learning and deep learning. Lab 5\n","## Competition\n","No competition for today\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning a model on a translation task\n","Today we will be finetunning T5(Text-To-Text Transfer Transformer) [model](https://github.com/google-research/t5x) on translation task. For this purpose we will be using [HuggingFace transformers](https://huggingface.co/docs/transformers/index) and [WMT16](https://huggingface.co/datasets/wmt16) dataset. "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"MOsHUjgdIrIW","outputId":"f84a093e-147f-470e-aad9-80fb51193c8e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (2.14.5)\n","Requirement already satisfied: sacrebleu in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (2.3.1)\n","Requirement already satisfied: transformers[sentencepiece] in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (4.34.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (1.24.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (13.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (2.1.1)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (0.17.3)\n","Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from datasets) (6.0)\n","Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (3.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (2023.10.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (0.4.0)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from transformers[sentencepiece]) (4.24.4)\n","Requirement already satisfied: portalocker in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (2.8.2)\n","Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from sacrebleu) (4.9.3)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: pywin32>=226 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: numpy==1.24.3 in c:\\users\\admin\\anaconda3\\envs\\cuda_transformers_temp\\lib\\site-packages (1.24.3)Note: you may need to restart the kernel to use updated packages.\n","\n"]}],"source":["# installing huggingface libraries for dataset, models and metrics\n","%pip install datasets transformers[sentencepiece] sacrebleu\n","\n","%pip install numpy==1.24.3"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.766013Z","iopub.status.busy":"2023-09-24T15:10:01.765366Z","iopub.status.idle":"2023-09-24T15:10:01.772400Z","shell.execute_reply":"2023-09-24T15:10:01.771384Z","shell.execute_reply.started":"2023-09-24T15:10:01.765977Z"},"trusted":true},"outputs":[],"source":["# Necessary inputs\n","import warnings\n","\n","from datasets import load_dataset, load_metric\n","import transformers\n","import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["## Selecting the model\n","For the example purpose we select as model checkpoint the smallest transformer in T5 family - `t5_small`. Other pre-trained models can be found [here](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=T5%20comes%20in%20different%20sizes%3A)."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.774778Z","iopub.status.busy":"2023-09-24T15:10:01.773897Z","iopub.status.idle":"2023-09-24T15:10:01.784448Z","shell.execute_reply":"2023-09-24T15:10:01.783216Z","shell.execute_reply.started":"2023-09-24T15:10:01.774744Z"},"trusted":true},"outputs":[],"source":["# selecting model checkpoint\n","model_checkpoint = \"t5-small\""]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.787917Z","iopub.status.busy":"2023-09-24T15:10:01.787594Z","iopub.status.idle":"2023-09-24T15:10:03.219266Z","shell.execute_reply":"2023-09-24T15:10:03.218277Z","shell.execute_reply.started":"2023-09-24T15:10:01.787893Z"},"id":"IreSlFmlIrIm","trusted":true},"outputs":[],"source":["# setting random seed for transformers library\n","transformers.set_seed(42)\n","\n","# Load the WMT16 dataset\n","raw_datasets = load_dataset(\"wmt16\", \"de-en\")\n","\n","# Load the BLUE metric\n","metric = load_metric(\"sacrebleu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset\n","Downloaded from HuggingFace dataset is a `DatasetDict`. It contains keys `[\"train\", \"validation\", \"test\"]` - which represents a dataset splits"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.220970Z","iopub.status.busy":"2023-09-24T15:10:03.220604Z","iopub.status.idle":"2023-09-24T15:10:03.229080Z","shell.execute_reply":"2023-09-24T15:10:03.228070Z","shell.execute_reply.started":"2023-09-24T15:10:03.220925Z"},"id":"GWiVUF0jIrIv","outputId":"35e3ea43-f397-4a54-c90c-f2cf8d36873e","trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['translation'],\n","        num_rows: 4548885\n","    })\n","    validation: Dataset({\n","        features: ['translation'],\n","        num_rows: 2169\n","    })\n","    test: Dataset({\n","        features: ['translation'],\n","        num_rows: 2999\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:35:02.763581Z","iopub.status.busy":"2023-09-24T15:35:02.762411Z","iopub.status.idle":"2023-09-24T15:35:02.772979Z","shell.execute_reply":"2023-09-24T15:35:02.771223Z","shell.execute_reply.started":"2023-09-24T15:35:02.763514Z"},"id":"X6HrpprwIrIz","outputId":"d7670bc0-42e4-4c09-8a6a-5c018ded7d95","trusted":true},"outputs":[{"data":{"text/plain":["{'translation': [{'de': 'Wiederaufnahme der Sitzungsperiode',\n","   'en': 'Resumption of the session'},\n","  {'de': 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.',\n","   'en': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'},\n","  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.',\n","   'en': \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"},\n","  {'de': 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.',\n","   'en': 'You have requested a debate on this subject in the course of the next few days, during this part-session.'},\n","  {'de': 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.',\n","   'en': \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"}]}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# samples from train dataset\n","raw_datasets[\"train\"][:5]"]},{"cell_type":"markdown","metadata":{},"source":["## Metric\n","[Sacrebleu](https://huggingface.co/spaces/evaluate-metric/sacrebleu) computes:\n","- `score`: BLEU score\n","- `counts`: list of counts of correct n-grams\n","- `totals`: list of counts of total n-grams\n","- `precisions`: list of precisions\n","- `bp`: Brevity penalty\n","- `sys_len`: cumulative sysem length\n","- `ref_len`: cumulative reference length\n","\n","The main metric is [BLEU score](https://en.wikipedia.org/wiki/BLEU). BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score measures the similarity of the machine-translated text to a set of high quality reference translations.\n","\n","The BLEU metric is calculates using [n-grams](https://en.wikipedia.org/wiki/N-gram)."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:38:54.393583Z","iopub.status.busy":"2023-09-24T15:38:54.393131Z","iopub.status.idle":"2023-09-24T15:38:54.403154Z","shell.execute_reply":"2023-09-24T15:38:54.401952Z","shell.execute_reply.started":"2023-09-24T15:38:54.393549Z"},"id":"5o4rUteaIrI_","outputId":"18038ef5-554c-45c5-e00a-133b02ec10f1","trusted":true},"outputs":[{"data":{"text/plain":["Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n","Produces BLEU scores along with its sufficient statistics\n","from a source against one or more references.\n","\n","Args:\n","    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n","    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n","    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n","        - `'none'`: no smoothing\n","        - `'floor'`: increment zero counts\n","        - `'add-k'`: increment num/denom by k for n>1\n","        - `'exp'`: exponential decay\n","    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n","    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n","        - `'none'`: No tokenization.\n","        - `'zh'`: Chinese tokenization.\n","        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n","        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n","        - `'char'`: Language-agnostic character-level tokenization.\n","        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n","    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n","    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n","    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n","\n","Returns:\n","    'score': BLEU score,\n","    'counts': Counts,\n","    'totals': Totals,\n","    'precisions': Precisions,\n","    'bp': Brevity penalty,\n","    'sys_len': predictions length,\n","    'ref_len': reference length,\n","\n","Examples:\n","\n","    Example 1:\n","        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n","        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        100.0\n","\n","    Example 2:\n","        >>> predictions = [\"hello there general kenobi\",\n","        ...                 \"on our way to ankh morpork\"]\n","        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n","        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n","        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n","        >>> results = sacrebleu.compute(predictions=predictions,\n","        ...                             references=references)\n","        >>> print(list(results.keys()))\n","        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","        >>> print(round(results[\"score\"], 1))\n","        39.8\n","\"\"\", stored examples: 0)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# how to use sacrebleu and its purpose\n","metric"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:05:06.585010Z","iopub.status.busy":"2023-09-24T16:05:06.584619Z","iopub.status.idle":"2023-09-24T16:05:06.601506Z","shell.execute_reply":"2023-09-24T16:05:06.600239Z","shell.execute_reply.started":"2023-09-24T16:05:06.584982Z"},"id":"6XN1Rq0aIrJC","outputId":"a4405435-a8a9-41ff-9f79-a13077b587c7","trusted":true},"outputs":[{"data":{"text/plain":["{'score': 45.59274666224604,\n"," 'counts': [7, 4, 1, 0],\n"," 'totals': [9, 6, 3, 2],\n"," 'precisions': [77.77777777777777,\n","  66.66666666666667,\n","  33.333333333333336,\n","  25.0],\n"," 'bp': 1.0,\n"," 'sys_len': 9,\n"," 'ref_len': 9}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["fake_preds = [\"hello there\", \"general kenobi\", \"Can I get an A\"]\n","fake_labels = [[\"hello there\"], [\"general kenobi\"], ['Can I get a C']]\n","metric.compute(predictions=fake_preds, references=fake_labels)"]},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["## Preprocessing the data\n","As usual we will need to preprocess data and tokenize it before passing to model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.310034Z","iopub.status.busy":"2023-09-24T15:10:03.309664Z","iopub.status.idle":"2023-09-24T15:10:03.505289Z","shell.execute_reply":"2023-09-24T15:10:03.504208Z","shell.execute_reply.started":"2023-09-24T15:10:03.310001Z"},"id":"eXNLu_-nIrJI","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# we will use autotokenizer for this purpose\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.507243Z","iopub.status.busy":"2023-09-24T15:10:03.506858Z","iopub.status.idle":"2023-09-24T15:10:03.514992Z","shell.execute_reply":"2023-09-24T15:10:03.513936Z","shell.execute_reply.started":"2023-09-24T15:10:03.507207Z"},"id":"a5hBlsrHIrJL","outputId":"acdaa98a-a8cd-4a20-89b8-cc26437bbe90","trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [8774, 6, 48, 80, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Hello, this one sentence!\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.517405Z","iopub.status.busy":"2023-09-24T15:10:03.516686Z","iopub.status.idle":"2023-09-24T15:10:03.529526Z","shell.execute_reply":"2023-09-24T15:10:03.528540Z","shell.execute_reply.started":"2023-09-24T15:10:03.517370Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.533132Z","iopub.status.busy":"2023-09-24T15:10:03.532860Z","iopub.status.idle":"2023-09-24T15:10:03.539292Z","shell.execute_reply":"2023-09-24T15:10:03.538145Z","shell.execute_reply.started":"2023-09-24T15:10:03.533109Z"},"trusted":true},"outputs":[],"source":["# prefix for model input\n","prefix = \"translate English to Deutsch:\""]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.541473Z","iopub.status.busy":"2023-09-24T15:10:03.540621Z","iopub.status.idle":"2023-09-24T15:10:03.549770Z","shell.execute_reply":"2023-09-24T15:10:03.548874Z","shell.execute_reply.started":"2023-09-24T15:10:03.541440Z"},"id":"vc0BSBLIIrJQ","trusted":true},"outputs":[],"source":["max_input_length = 128\n","max_target_length = 128\n","source_lang = \"en\"\n","target_lang = \"de\"\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n","    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.551620Z","iopub.status.busy":"2023-09-24T15:10:03.551106Z","iopub.status.idle":"2023-09-24T15:10:03.571019Z","shell.execute_reply":"2023-09-24T15:10:03.569879Z","shell.execute_reply.started":"2023-09-24T15:10:03.551587Z"},"id":"-b70jh26IrJS","outputId":"acd3a42d-985b-44ee-9daa-af5d944ce1d9","trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [[13959, 1566, 12, 3, 18609, 10, 1649, 4078, 102, 1575, 13, 8, 2363, 1], [13959, 1566, 12, 3, 18609, 10, 196, 15884, 4258, 26, 8, 2363, 13, 8, 1611, 12876, 19181, 1211, 29, 15, 26, 30, 1701, 1003, 1882, 5247, 6, 11, 27, 133, 114, 728, 541, 12, 1663, 25, 3, 9, 1095, 126, 215, 16, 8, 897, 24, 25, 2994, 3, 9, 8714, 15723, 1059, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[15158, 24860, 74, 11216, 425, 7, 4267, 32, 221, 1], [1674, 3, 49, 20635, 15, 67, 183, 17874, 6, 340, 11030, 17900, 1199, 5702, 1559, 15, 11216, 425, 7, 4267, 32, 221, 93, 3, 30604, 29, 13636, 7, 218, 1403, 3019, 7026, 6, 3, 25084, 2587, 18794, 7, 3532, 7756, 15, 674, 9242, 11621, 64, 3, 11950, 15, 6, 3, 26, 7118, 292, 11878, 16849, 8827, 5, 1]]}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# example of preprocessing\n","preprocess_function(raw_datasets['train'][:2])"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:09:35.662567Z","iopub.status.busy":"2023-09-24T16:09:35.662099Z","iopub.status.idle":"2023-09-24T16:09:37.545512Z","shell.execute_reply":"2023-09-24T16:09:37.544516Z","shell.execute_reply.started":"2023-09-24T16:09:35.662533Z"},"id":"DDtsaJeVIrJT","outputId":"aa4734bf-4ef5-4437-9948-2c16363da719","trusted":true},"outputs":[{"data":{"text/plain":["{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode',\n","  'en': 'Resumption of the session'},\n"," 'input_ids': [13959,\n","  1566,\n","  12,\n","  3,\n","  18609,\n","  10,\n","  1649,\n","  4078,\n","  102,\n","  1575,\n","  13,\n","  8,\n","  2363,\n","  1],\n"," 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n"," 'labels': [15158, 24860, 74, 11216, 425, 7, 4267, 32, 221, 1]}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# for the example purpose we will crop the dataset and select first 5000 for train\n","# and 500 for validation and test\n","cropped_datasets = raw_datasets\n","cropped_datasets['train'] = raw_datasets['train'].select(range(5000))\n","cropped_datasets['validation'] = raw_datasets['validation'].select(range(500))\n","cropped_datasets['test'] = raw_datasets['test'].select(range(500))\n","tokenized_datasets = cropped_datasets.map(preprocess_function, batched=True)\n","tokenized_datasets['train'][0]"]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Fine-tuning the model"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.605060Z","iopub.status.busy":"2023-09-24T15:10:03.604730Z","iopub.status.idle":"2023-09-24T15:10:04.671893Z","shell.execute_reply":"2023-09-24T15:10:04.670859Z","shell.execute_reply.started":"2023-09-24T15:10:03.605029Z"},"id":"TlqNaB8jIrJW","outputId":"84916cf3-6e6c-47f3-d081-032ec30a4132","trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","# create a model for the pretrained model\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.674163Z","iopub.status.busy":"2023-09-24T15:10:04.673474Z","iopub.status.idle":"2023-09-24T15:10:04.681771Z","shell.execute_reply":"2023-09-24T15:10:04.680562Z","shell.execute_reply.started":"2023-09-24T15:10:04.674126Z"},"id":"Bliy8zgjIrJY","trusted":true},"outputs":[],"source":["# defining the parameters for training\n","batch_size = 32\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=10,\n","    predict_with_generate=True,\n","    fp16=True,\n","    report_to='tensorboard',\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.684376Z","iopub.status.busy":"2023-09-24T15:10:04.683883Z","iopub.status.idle":"2023-09-24T15:10:04.693774Z","shell.execute_reply":"2023-09-24T15:10:04.692863Z","shell.execute_reply.started":"2023-09-24T15:10:04.684341Z"},"trusted":true},"outputs":[],"source":["# instead of writing collate_fn function we will use DataCollatorForSeq2Seq\n","# simliarly it implements the batch creation for training\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.695838Z","iopub.status.busy":"2023-09-24T15:10:04.695457Z","iopub.status.idle":"2023-09-24T15:10:04.707222Z","shell.execute_reply":"2023-09-24T15:10:04.706315Z","shell.execute_reply.started":"2023-09-24T15:10:04.695806Z"},"id":"UmvbnJ9JIrJd","trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# simple postprocessing for text\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels\n","\n","# compute metrics function to pass to trainer\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    \n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.709249Z","iopub.status.busy":"2023-09-24T15:10:04.708526Z","iopub.status.idle":"2023-09-24T15:10:04.806768Z","shell.execute_reply":"2023-09-24T15:10:04.805816Z","shell.execute_reply.started":"2023-09-24T15:10:04.709216Z"},"id":"imY1oC3SIrJf","trusted":true},"outputs":[],"source":["# instead of writing train loop we will use Seq2SeqTrainer\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:04.808506Z","iopub.status.busy":"2023-09-24T15:10:04.808050Z","iopub.status.idle":"2023-09-24T15:18:44.110261Z","shell.execute_reply":"2023-09-24T15:18:44.109150Z","shell.execute_reply.started":"2023-09-24T15:10:04.808459Z"},"id":"uNx5pyRlIrJh","outputId":"077e661e-d36c-469b-89b8-7ff7f73541ec","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f38b73c7ab2545d689024b151542c0dc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1570 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c642f84b5b640bd8682c7538bb05bbd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3392888307571411, 'eval_bleu': 10.9899, 'eval_gen_len': 17.642, 'eval_runtime': 4.8328, 'eval_samples_per_second': 103.461, 'eval_steps_per_second': 3.311, 'epoch': 1.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2919b5613674cc9acae434fabed5580","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3392494916915894, 'eval_bleu': 10.8806, 'eval_gen_len': 17.65, 'eval_runtime': 4.7989, 'eval_samples_per_second': 104.19, 'eval_steps_per_second': 3.334, 'epoch': 2.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b4fc4cb1e0744b08b0934c0d9d21097","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3395845890045166, 'eval_bleu': 10.8545, 'eval_gen_len': 17.602, 'eval_runtime': 5.1505, 'eval_samples_per_second': 97.079, 'eval_steps_per_second': 3.107, 'epoch': 3.0}\n","{'loss': 1.3208, 'learning_rate': 1.3630573248407644e-05, 'epoch': 3.18}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9308f0f609ed456881fc74f7e8a49f07","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3398561477661133, 'eval_bleu': 10.7283, 'eval_gen_len': 17.618, 'eval_runtime': 7.5093, 'eval_samples_per_second': 66.584, 'eval_steps_per_second': 2.131, 'epoch': 4.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ae5e3444beb4bf9ac1784ebcb43aa26","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3402308225631714, 'eval_bleu': 10.7102, 'eval_gen_len': 17.604, 'eval_runtime': 4.4612, 'eval_samples_per_second': 112.078, 'eval_steps_per_second': 3.587, 'epoch': 5.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ad124d9329442edaa9bff9705e7beac","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3409031629562378, 'eval_bleu': 10.7356, 'eval_gen_len': 17.598, 'eval_runtime': 4.8697, 'eval_samples_per_second': 102.677, 'eval_steps_per_second': 3.286, 'epoch': 6.0}\n","{'loss': 1.2741, 'learning_rate': 7.261146496815287e-06, 'epoch': 6.37}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a2b5dca5d5c417b9a67f4b9de9949ad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.341385841369629, 'eval_bleu': 10.7952, 'eval_gen_len': 17.596, 'eval_runtime': 4.6651, 'eval_samples_per_second': 107.178, 'eval_steps_per_second': 3.43, 'epoch': 7.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d6f6dae53b64bc68927dbcaabc9f7bc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3417431116104126, 'eval_bleu': 10.7706, 'eval_gen_len': 17.59, 'eval_runtime': 6.8932, 'eval_samples_per_second': 72.535, 'eval_steps_per_second': 2.321, 'epoch': 8.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"888430e0482d42aeb043b57d07586fd5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.341884970664978, 'eval_bleu': 10.8015, 'eval_gen_len': 17.584, 'eval_runtime': 4.5162, 'eval_samples_per_second': 110.711, 'eval_steps_per_second': 3.543, 'epoch': 9.0}\n","{'loss': 1.2634, 'learning_rate': 9.044585987261147e-07, 'epoch': 9.55}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b288daeae2b042959f8b4937ad868ad2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.342007040977478, 'eval_bleu': 10.8123, 'eval_gen_len': 17.584, 'eval_runtime': 4.6832, 'eval_samples_per_second': 106.765, 'eval_steps_per_second': 3.416, 'epoch': 10.0}\n","{'train_runtime': 328.8671, 'train_samples_per_second': 152.037, 'train_steps_per_second': 4.774, 'train_loss': 1.2848550687170333, 'epoch': 10.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=1570, training_loss=1.2848550687170333, metrics={'train_runtime': 328.8671, 'train_samples_per_second': 152.037, 'train_steps_per_second': 4.774, 'train_loss': 1.2848550687170333, 'epoch': 10.0})"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:19:29.403450Z","iopub.status.busy":"2023-09-24T15:19:29.403061Z","iopub.status.idle":"2023-09-24T15:19:30.003295Z","shell.execute_reply":"2023-09-24T15:19:30.002182Z","shell.execute_reply.started":"2023-09-24T15:19:29.403420Z"},"trusted":true},"outputs":[],"source":["# saving model\n","trainer.save_model('best')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:19:30.753608Z","iopub.status.busy":"2023-09-24T15:19:30.753167Z","iopub.status.idle":"2023-09-24T15:19:31.676057Z","shell.execute_reply":"2023-09-24T15:19:31.675005Z","shell.execute_reply.started":"2023-09-24T15:19:30.753575Z"},"trusted":true},"outputs":[],"source":["# loading the model and run inference for it\n","model = AutoModelForSeq2SeqLM.from_pretrained('best')\n","model.eval()\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:19:31.744595Z","iopub.status.busy":"2023-09-24T15:19:31.744257Z","iopub.status.idle":"2023-09-24T15:19:31.749935Z","shell.execute_reply":"2023-09-24T15:19:31.748926Z","shell.execute_reply.started":"2023-09-24T15:19:31.744568Z"},"trusted":true},"outputs":[],"source":["def translate(model, inference_request, tokenizer=tokenizer):\n","    input_ids = tokenizer(inference_request, return_tensors=\"pt\").input_ids\n","    outputs = model.generate(input_ids=input_ids)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True,temperature=0))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:13:02.462385Z","iopub.status.busy":"2023-09-24T16:13:02.461389Z","iopub.status.idle":"2023-09-24T16:13:02.943140Z","shell.execute_reply":"2023-09-24T16:13:02.941981Z","shell.execute_reply.started":"2023-09-24T16:13:02.462348Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Warum hat es so lange dauern, das Modell zu trainieren?\n"]}],"source":["inference_request = prefix + 'Why did it take so long to train the model?'\n","translate(model, inference_request,tokenizer)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:13:10.223355Z","iopub.status.busy":"2023-09-24T16:13:10.222968Z","iopub.status.idle":"2023-09-24T16:13:10.549431Z","shell.execute_reply":"2023-09-24T16:13:10.548426Z","shell.execute_reply.started":"2023-09-24T16:13:10.223326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mein Name ist Wolfgang und ich lebe in Berlin.\n"]}],"source":["inference_request = prefix +\"My name is Wolfgang and I live in Berlin\"\n","translate(model, inference_request,tokenizer)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T16:17:24.580846Z","iopub.status.busy":"2023-09-24T16:17:24.580402Z","iopub.status.idle":"2023-09-24T16:17:24.852369Z","shell.execute_reply":"2023-09-24T16:17:24.851186Z","shell.execute_reply.started":"2023-09-24T16:17:24.580812Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ihre Aufgabe ist schwierig, beginnen Sie heute\n"]}],"source":["inference_request = prefix + \"Your assignment is hard. Start it today\"\n","translate(model, inference_request,tokenizer)"]}],"metadata":{"colab":{"name":"Translation","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
