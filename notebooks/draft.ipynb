{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.766013Z","iopub.status.busy":"2023-09-24T15:10:01.765366Z","iopub.status.idle":"2023-09-24T15:10:01.772400Z","shell.execute_reply":"2023-09-24T15:10:01.771384Z","shell.execute_reply.started":"2023-09-24T15:10:01.765977Z"},"trusted":true},"outputs":[],"source":["import transformers\n","import torch\n","import random\n","import numpy as np\n","from torch.utils.data import random_split\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import GenerationConfig\n","\n","import sys\n","if '../' not in sys.path: sys.path.insert(1, '../')\n","from src.data.make_dataset import load_detoxification_dataset, load_toxicity_dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Load the pretrained T5"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["global_seed = 1984\n","\n","transformers.set_seed(global_seed)\n","random.seed(global_seed)\n","np.random.seed(global_seed)\n","torch.manual_seed(global_seed)\n","torch.cuda.manual_seed_all(global_seed)\n","model_checkpoint = \"t5-small\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","dataset_portion = 0.01\n","dataset_kwargs = {\n","    'path': '../data/raw/filtered.tsv', # path to raw data\n","    'cache_path': '../data/processed/tokenized.tsv', # path to processed data (or where to store it)\n","    'tokenizer': tokenizer, # tokenizer to tokenize texts\n","    'portion': dataset_portion # get only a portion of dataset [0..1]\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["dataset = load_detoxification_dataset(**dataset_kwargs)\n","\n","val_ratio = 0.2\n","train_dataset, val_dataset = random_split(dataset, [1 - val_ratio, val_ratio])"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# defining the parameters for training\n","genConfig = GenerationConfig.from_pretrained(model_checkpoint)\n","genConfig.max_new_tokens = 64\n","\n","batch_size = 32\n","postfix = \"-10\"\n","save_model_path = f'../models/t5_detoxifier{postfix}'\n","args = Seq2SeqTrainingArguments(\n","    f\"../models/{model_checkpoint}-detoxification{postfix}\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=5,\n","    predict_with_generate=True,\n","    fp16=True,\n","    report_to='tensorboard',\n","    logging_steps=5000,\n","    save_steps=10000,\n","    generation_config=genConfig\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=collator,\n","    tokenizer=tokenizer,\n","    # compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa667e315b5b46eeb342626681d87113","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/725 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad496ce4a09d4d9cae0dcfb9ffcad084","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 2.121922016143799, 'eval_runtime': 0.9647, 'eval_samples_per_second': 1197.224, 'eval_steps_per_second': 38.353, 'epoch': 1.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42bbd97eb8db4923ac5678c9fa777ba9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 2.0536093711853027, 'eval_runtime': 1.0399, 'eval_samples_per_second': 1110.679, 'eval_steps_per_second': 35.58, 'epoch': 2.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2629e217e711458c8cd4e03cb24980aa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 2.027437686920166, 'eval_runtime': 0.9848, 'eval_samples_per_second': 1172.771, 'eval_steps_per_second': 37.569, 'epoch': 3.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70283abaed364ab49fdd1c04d2f3944f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 2.013796091079712, 'eval_runtime': 1.0278, 'eval_samples_per_second': 1123.806, 'eval_steps_per_second': 36.001, 'epoch': 4.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c377cb3bd52343cfaf63dcac2e3e65c6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/37 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 2.0110414028167725, 'eval_runtime': 1.0049, 'eval_samples_per_second': 1149.388, 'eval_steps_per_second': 36.82, 'epoch': 5.0}\n","{'train_runtime': 77.9113, 'train_samples_per_second': 296.619, 'train_steps_per_second': 9.305, 'train_loss': 2.274500942887931, 'epoch': 5.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=725, training_loss=2.274500942887931, metrics={'train_runtime': 77.9113, 'train_samples_per_second': 296.619, 'train_steps_per_second': 9.305, 'train_loss': 2.274500942887931, 'epoch': 5.0})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# saving model\n","trainer.save_model(save_model_path)"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# loading the model and run inference for it\n","# model = AutoModelForSeq2SeqLM.from_pretrained(save_model_path)\n","model = AutoModelForSeq2SeqLM.from_pretrained('../models backup/t5_detoxifier-10')\n","model.eval()\n","model.config.use_cache = False"]},{"cell_type":"markdown","metadata":{},"source":["# Testing ??"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["def translate(model, inference_request, tokenizer=tokenizer):\n","    tokenized = tokenizer.encode(inference_request, return_tensors=\"pt\")\n","    outputs = model.generate(tokenized, generation_config=genConfig)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["This guy is a con, he's an actor, he's just a character.\n"]}],"source":["inference_request = \"\"\"\n","this guy is a con man. He's an actor. He's just a character.\n","\"\"\"\n","translate(model, inference_request)"]},{"cell_type":"markdown","metadata":{},"source":["# Validation ????"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from src.models.t5_toxicity_evaluator import T5TEModel\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","evalutator = T5TEModel('../models/last_toxic_regressor/model.pt').to(device)\n","model.to(device)\n","_ = evalutator.model.eval()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["eval_dataset = load_toxicity_dataset(**dataset_kwargs)\n","eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=128, shuffle=False, collate_fn=evalutator.collate_batch)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b5498cc5f7d4fd8a955d74b59fe13a4","version_major":2,"version_minor":0},"text/plain":["Translating:   0%|          | 0/452 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}],"source":["from tqdm.auto import tqdm\n","transformed = []\n","\n","for batch in tqdm(eval_loader, total=len(eval_loader), desc='Translating'):\n","    output = model.generate(input_ids=batch.input_ids, attention_mask=batch.attention_mask, generation_config=genConfig)\n","    transformed += output.detach().cpu()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6125139129804d19b8ecfdb237b35585","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/452 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["ref_evaluations = []\n","\n","torch.cuda.empty_cache()\n","for batch in tqdm(eval_loader, total=len(eval_loader), desc='Evaluation'):\n","    output = evalutator(batch)\n","    ref_evaluations += output.detach().cpu()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["transformed_keys = [{'input_ids': x} for x in transformed]\n","trn_loader = torch.utils.data.DataLoader(transformed_keys, batch_size=128, shuffle=False, collate_fn=evalutator.collate_batch)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cccd4f9606c74cd58acf778cb0f7715b","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/452 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["trn_evaluations = []\n","\n","torch.cuda.empty_cache()\n","for batch in tqdm(trn_loader, total=len(trn_loader), desc='Evaluation'):\n","    output = evalutator(batch)\n","    trn_evaluations += output.detach().cpu()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["threshold = 0.5\n","\n","refevs = np.array(ref_evaluations)\n","trnevs = np.array(trn_evaluations)\n","\n","ref_toxs = refevs > threshold\n","trn_toxs = trnevs > threshold"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Neutral -> neutral: 27871 -> 27857\n","Neutral -> toxic: 27871 -> 14\n","Toxic -> neutral: 29906 -> 28996\n","Toxic -> toxic: 29906 -> 910\n"]}],"source":["ref_neutrals = ref_toxs == False\n","ref_toxics = ref_toxs == True\n","trn_neutrals = trn_toxs == False\n","trn_toxics = trn_toxs == True\n","\n","print(f'Neutral -> neutral: {np.sum(ref_neutrals)} -> {np.sum(np.logical_and(ref_neutrals, trn_neutrals))}')\n","print(f'Neutral -> toxic: {np.sum(ref_neutrals)} -> {np.sum(np.logical_and(ref_neutrals, trn_toxics))}')\n","print(f'Toxic -> neutral: {np.sum(ref_toxics)} -> {np.sum(np.logical_and(ref_toxics, trn_neutrals))}')\n","print(f'Toxic -> toxic: {np.sum(ref_toxics)} -> {np.sum(np.logical_and(ref_toxics, trn_toxics))}')"]}],"metadata":{"colab":{"name":"Translation","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
