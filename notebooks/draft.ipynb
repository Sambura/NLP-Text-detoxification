{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.766013Z","iopub.status.busy":"2023-09-24T15:10:01.765366Z","iopub.status.idle":"2023-09-24T15:10:01.772400Z","shell.execute_reply":"2023-09-24T15:10:01.771384Z","shell.execute_reply.started":"2023-09-24T15:10:01.765977Z"},"trusted":true},"outputs":[],"source":["# Necessary inputs\n","import warnings\n","\n","from datasets import load_dataset, load_metric\n","import transformers\n","import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","import torch\n","from torch.utils.data import random_split\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["## Selecting the model\n","For the example purpose we select as model checkpoint the smallest transformer in T5 family - `t5_small`. Other pre-trained models can be found [here](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=T5%20comes%20in%20different%20sizes%3A)."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.774778Z","iopub.status.busy":"2023-09-24T15:10:01.773897Z","iopub.status.idle":"2023-09-24T15:10:01.784448Z","shell.execute_reply":"2023-09-24T15:10:01.783216Z","shell.execute_reply.started":"2023-09-24T15:10:01.774744Z"},"trusted":true},"outputs":[],"source":["# selecting model checkpoint\n","model_checkpoint = \"t5-small\""]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset (???)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reference</th>\n","      <th>translation</th>\n","      <th>similarity</th>\n","      <th>lenght_diff</th>\n","      <th>ref_tox</th>\n","      <th>trn_tox</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>If Alkar is flooding her with psychic waste, t...</td>\n","      <td>if Alkar floods her with her mental waste, it ...</td>\n","      <td>0.785171</td>\n","      <td>0.010309</td>\n","      <td>0.014195</td>\n","      <td>0.981983</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Now you're getting nasty.</td>\n","      <td>you're becoming disgusting.</td>\n","      <td>0.749687</td>\n","      <td>0.071429</td>\n","      <td>0.065473</td>\n","      <td>0.999039</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Well, we could spare your life, for one.</td>\n","      <td>well, we can spare your life.</td>\n","      <td>0.919051</td>\n","      <td>0.268293</td>\n","      <td>0.213313</td>\n","      <td>0.985068</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ah! Monkey, you've got to snap out of it.</td>\n","      <td>monkey, you have to wake up.</td>\n","      <td>0.664333</td>\n","      <td>0.309524</td>\n","      <td>0.053362</td>\n","      <td>0.994215</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I've got orders to put her down.</td>\n","      <td>I have orders to kill her.</td>\n","      <td>0.726639</td>\n","      <td>0.181818</td>\n","      <td>0.009402</td>\n","      <td>0.999348</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>577772</th>\n","      <td>You didn't know that Estelle had stolen some f...</td>\n","      <td>you didn't know that Estelle stole your fish f...</td>\n","      <td>0.870322</td>\n","      <td>0.030769</td>\n","      <td>0.000121</td>\n","      <td>0.949143</td>\n","    </tr>\n","    <tr>\n","      <th>577773</th>\n","      <td>It'il suck the life out of you!</td>\n","      <td>you'd be sucked out of your life!</td>\n","      <td>0.722897</td>\n","      <td>0.058824</td>\n","      <td>0.996124</td>\n","      <td>0.215794</td>\n","    </tr>\n","    <tr>\n","      <th>577774</th>\n","      <td>I can't fuckin' take that, bruv.</td>\n","      <td>I really can't take this.</td>\n","      <td>0.617511</td>\n","      <td>0.212121</td>\n","      <td>0.984538</td>\n","      <td>0.000049</td>\n","    </tr>\n","    <tr>\n","      <th>577775</th>\n","      <td>They called me a fucking hero. The truth is I ...</td>\n","      <td>they said I was a hero, but I didn't care.</td>\n","      <td>0.679613</td>\n","      <td>0.358209</td>\n","      <td>0.991945</td>\n","      <td>0.000124</td>\n","    </tr>\n","    <tr>\n","      <th>577776</th>\n","      <td>I did not screw him.</td>\n","      <td>I didn't fuck him.</td>\n","      <td>0.868475</td>\n","      <td>0.095238</td>\n","      <td>0.009480</td>\n","      <td>0.994174</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>577777 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                reference  \\\n","0       If Alkar is flooding her with psychic waste, t...   \n","1                               Now you're getting nasty.   \n","2                Well, we could spare your life, for one.   \n","3               Ah! Monkey, you've got to snap out of it.   \n","4                        I've got orders to put her down.   \n","...                                                   ...   \n","577772  You didn't know that Estelle had stolen some f...   \n","577773                    It'il suck the life out of you!   \n","577774                   I can't fuckin' take that, bruv.   \n","577775  They called me a fucking hero. The truth is I ...   \n","577776                               I did not screw him.   \n","\n","                                              translation  similarity  \\\n","0       if Alkar floods her with her mental waste, it ...    0.785171   \n","1                             you're becoming disgusting.    0.749687   \n","2                           well, we can spare your life.    0.919051   \n","3                            monkey, you have to wake up.    0.664333   \n","4                              I have orders to kill her.    0.726639   \n","...                                                   ...         ...   \n","577772  you didn't know that Estelle stole your fish f...    0.870322   \n","577773                  you'd be sucked out of your life!    0.722897   \n","577774                          I really can't take this.    0.617511   \n","577775         they said I was a hero, but I didn't care.    0.679613   \n","577776                                 I didn't fuck him.    0.868475   \n","\n","        lenght_diff   ref_tox   trn_tox  \n","0          0.010309  0.014195  0.981983  \n","1          0.071429  0.065473  0.999039  \n","2          0.268293  0.213313  0.985068  \n","3          0.309524  0.053362  0.994215  \n","4          0.181818  0.009402  0.999348  \n","...             ...       ...       ...  \n","577772     0.030769  0.000121  0.949143  \n","577773     0.058824  0.996124  0.215794  \n","577774     0.212121  0.984538  0.000049  \n","577775     0.358209  0.991945  0.000124  \n","577776     0.095238  0.009480  0.994174  \n","\n","[577777 rows x 6 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df_raw = pd.read_csv('../data/raw/filtered.tsv', delimiter='\\t', index_col=0)\n","df_raw"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df = df_raw.drop(columns=[\"similarity\", \"lenght_diff\"])"]},{"cell_type":"markdown","metadata":{},"source":["Make reference always more toxic than the translation:"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reference</th>\n","      <th>translation</th>\n","      <th>ref_tox</th>\n","      <th>trn_tox</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>if Alkar floods her with her mental waste, it ...</td>\n","      <td>If Alkar is flooding her with psychic waste, t...</td>\n","      <td>0.981983</td>\n","      <td>0.014195</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>you're becoming disgusting.</td>\n","      <td>Now you're getting nasty.</td>\n","      <td>0.999039</td>\n","      <td>0.065473</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>well, we can spare your life.</td>\n","      <td>Well, we could spare your life, for one.</td>\n","      <td>0.985068</td>\n","      <td>0.213313</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>monkey, you have to wake up.</td>\n","      <td>Ah! Monkey, you've got to snap out of it.</td>\n","      <td>0.994215</td>\n","      <td>0.053362</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I have orders to kill her.</td>\n","      <td>I've got orders to put her down.</td>\n","      <td>0.999348</td>\n","      <td>0.009402</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>577772</th>\n","      <td>you didn't know that Estelle stole your fish f...</td>\n","      <td>You didn't know that Estelle had stolen some f...</td>\n","      <td>0.949143</td>\n","      <td>0.000121</td>\n","    </tr>\n","    <tr>\n","      <th>577773</th>\n","      <td>It'il suck the life out of you!</td>\n","      <td>you'd be sucked out of your life!</td>\n","      <td>0.996124</td>\n","      <td>0.215794</td>\n","    </tr>\n","    <tr>\n","      <th>577774</th>\n","      <td>I can't fuckin' take that, bruv.</td>\n","      <td>I really can't take this.</td>\n","      <td>0.984538</td>\n","      <td>0.000049</td>\n","    </tr>\n","    <tr>\n","      <th>577775</th>\n","      <td>They called me a fucking hero. The truth is I ...</td>\n","      <td>they said I was a hero, but I didn't care.</td>\n","      <td>0.991945</td>\n","      <td>0.000124</td>\n","    </tr>\n","    <tr>\n","      <th>577776</th>\n","      <td>I didn't fuck him.</td>\n","      <td>I did not screw him.</td>\n","      <td>0.994174</td>\n","      <td>0.009480</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>577777 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                reference  \\\n","0       if Alkar floods her with her mental waste, it ...   \n","1                             you're becoming disgusting.   \n","2                           well, we can spare your life.   \n","3                            monkey, you have to wake up.   \n","4                              I have orders to kill her.   \n","...                                                   ...   \n","577772  you didn't know that Estelle stole your fish f...   \n","577773                    It'il suck the life out of you!   \n","577774                   I can't fuckin' take that, bruv.   \n","577775  They called me a fucking hero. The truth is I ...   \n","577776                                 I didn't fuck him.   \n","\n","                                              translation   ref_tox   trn_tox  \n","0       If Alkar is flooding her with psychic waste, t...  0.981983  0.014195  \n","1                               Now you're getting nasty.  0.999039  0.065473  \n","2                Well, we could spare your life, for one.  0.985068  0.213313  \n","3               Ah! Monkey, you've got to snap out of it.  0.994215  0.053362  \n","4                        I've got orders to put her down.  0.999348  0.009402  \n","...                                                   ...       ...       ...  \n","577772  You didn't know that Estelle had stolen some f...  0.949143  0.000121  \n","577773                  you'd be sucked out of your life!  0.996124  0.215794  \n","577774                          I really can't take this.  0.984538  0.000049  \n","577775         they said I was a hero, but I didn't care.  0.991945  0.000124  \n","577776                               I did not screw him.  0.994174  0.009480  \n","\n","[577777 rows x 4 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["mask = df['ref_tox'] < df['trn_tox']\n","\n","df['translation'][mask], df['reference'][mask] = df['reference'][mask], df['translation'][mask]\n","df['ref_tox'][mask], df['trn_tox'][mask] = df['trn_tox'][mask], df['ref_tox'][mask]\n","df"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reference</th>\n","      <th>translation</th>\n","      <th>ref_tox</th>\n","      <th>trn_tox</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>32</th>\n","      <td>and Murray has his eyes on his ass, he can't s...</td>\n","      <td>And Murray, being the butt-plug that he is, do...</td>\n","      <td>0.996764</td>\n","      <td>0.059791</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>Not content with merely killing my son, the Co...</td>\n","      <td>the Count and his bloodlust-starved nephews we...</td>\n","      <td>0.927254</td>\n","      <td>0.049472</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>That fool's gonna look up and see that check t...</td>\n","      <td>the poor driver looks at the payout that the c...</td>\n","      <td>0.998072</td>\n","      <td>0.000280</td>\n","    </tr>\n","    <tr>\n","      <th>124</th>\n","      <td>but it was a human race that put us all in thi...</td>\n","      <td>But it was the human race that had jammed us a...</td>\n","      <td>0.857898</td>\n","      <td>0.074480</td>\n","    </tr>\n","    <tr>\n","      <th>212</th>\n","      <td>It was as if God had decided Delacroix needed ...</td>\n","      <td>it seemed as if God had chosen to cast Delacro...</td>\n","      <td>0.920242</td>\n","      <td>0.001022</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>577552</th>\n","      <td>you know, just you and me... and that moment o...</td>\n","      <td>You know, it's just that you and I finally hav...</td>\n","      <td>0.859250</td>\n","      <td>0.028703</td>\n","    </tr>\n","    <tr>\n","      <th>577648</th>\n","      <td>Or perhaps the ancient communal traditions... ...</td>\n","      <td>or, perhaps, those ancient social traditions t...</td>\n","      <td>0.980451</td>\n","      <td>0.022242</td>\n","    </tr>\n","    <tr>\n","      <th>577711</th>\n","      <td>\"I don't see how you get so upset,\" Arnie answ...</td>\n","      <td>'I don't get what you're so upset about,' Arni...</td>\n","      <td>0.999119</td>\n","      <td>0.006092</td>\n","    </tr>\n","    <tr>\n","      <th>577718</th>\n","      <td>but in the midst of her astonishment, she hear...</td>\n","      <td>And then she was shockingly surprised to hear ...</td>\n","      <td>0.996422</td>\n","      <td>0.006014</td>\n","    </tr>\n","    <tr>\n","      <th>577744</th>\n","      <td>hentai amnesia ibiza sex scenes linkin park en...</td>\n","      <td>amnesia ibiza power rangers mistic forse amnes...</td>\n","      <td>0.968118</td>\n","      <td>0.000347</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13255 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                reference  \\\n","32      and Murray has his eyes on his ass, he can't s...   \n","40      Not content with merely killing my son, the Co...   \n","119     That fool's gonna look up and see that check t...   \n","124     but it was a human race that put us all in thi...   \n","212     It was as if God had decided Delacroix needed ...   \n","...                                                   ...   \n","577552  you know, just you and me... and that moment o...   \n","577648  Or perhaps the ancient communal traditions... ...   \n","577711  \"I don't see how you get so upset,\" Arnie answ...   \n","577718  but in the midst of her astonishment, she hear...   \n","577744  hentai amnesia ibiza sex scenes linkin park en...   \n","\n","                                              translation   ref_tox   trn_tox  \n","32      And Murray, being the butt-plug that he is, do...  0.996764  0.059791  \n","40      the Count and his bloodlust-starved nephews we...  0.927254  0.049472  \n","119     the poor driver looks at the payout that the c...  0.998072  0.000280  \n","124     But it was the human race that had jammed us a...  0.857898  0.074480  \n","212     it seemed as if God had chosen to cast Delacro...  0.920242  0.001022  \n","...                                                   ...       ...       ...  \n","577552  You know, it's just that you and I finally hav...  0.859250  0.028703  \n","577648  or, perhaps, those ancient social traditions t...  0.980451  0.022242  \n","577711  'I don't get what you're so upset about,' Arni...  0.999119  0.006092  \n","577718  And then she was shockingly surprised to hear ...  0.996422  0.006014  \n","577744  amnesia ibiza power rangers mistic forse amnes...  0.968118  0.000347  \n","\n","[13255 rows x 4 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","df = df[:][df['translation'].str.len() > 150]\n","df"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class DetoxificationDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataframe, tokenizer, prefix, max_length, return_scores=False, samples=-1):\n","        self.raw_data = dataframe.to_numpy()[:samples,:]\n","        self.return_scores = return_scores\n","\n","        inputs = (prefix + self.raw_data[:,0]).astype(str).tolist()\n","        targets = self.raw_data[:,1].tolist()\n","\n","        self.inputs = []\n","        for input, target in zip(inputs, targets):\n","            model_input = tokenizer(input, max_length=max_length, truncation=True)\n","            label = tokenizer(target, max_length=max_length, truncation=True)\n","            model_input['labels'] = label['input_ids']\n","            self.inputs.append(model_input)\n","\n","        #for input, label in zip(self.inputs, labels):\n","        #    input['labels'] = label[\"input_ids\"]\n","\n","    def __getitem__(self, idx):\n","        return self.inputs[idx]\n","\n","    def __len__(self):\n","        return len(self.inputs)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class LongerDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataframe, tokenizer, prefix, max_length, return_scores=False, samples=-1):\n","        self.raw_data = dataframe.to_numpy()[:samples,:]\n","        self.return_scores = return_scores\n","\n","        inputs = (prefix + self.raw_data[:,0]).astype(str).tolist()\n","        targets = self.raw_data[:,1].tolist()\n","\n","        self.inputs = []\n","        for input in inputs + targets:\n","            model_input = tokenizer(input, max_length=max_length, truncation=True)\n","            model_input['labels'] = model_input['input_ids']\n","            self.inputs.append(model_input)\n","\n","    def __getitem__(self, idx):\n","        return self.inputs[idx]\n","\n","    def __len__(self):\n","        return len(self.inputs)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:01.787917Z","iopub.status.busy":"2023-09-24T15:10:01.787594Z","iopub.status.idle":"2023-09-24T15:10:03.219266Z","shell.execute_reply":"2023-09-24T15:10:03.218277Z","shell.execute_reply.started":"2023-09-24T15:10:01.787893Z"},"id":"IreSlFmlIrIm","trusted":true},"outputs":[],"source":["global_seed = 1984\n","# setting random seed for transformers library\n","transformers.set_seed(global_seed)"]},{"cell_type":"markdown","metadata":{"id":"n9qywopnIrJH"},"source":["## Preprocessing the data\n","As usual we will need to preprocess data and tokenize it before passing to model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.310034Z","iopub.status.busy":"2023-09-24T15:10:03.309664Z","iopub.status.idle":"2023-09-24T15:10:03.505289Z","shell.execute_reply":"2023-09-24T15:10:03.504208Z","shell.execute_reply.started":"2023-09-24T15:10:03.310001Z"},"id":"eXNLu_-nIrJI","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# we will use autotokenizer for this purpose\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","prefix = \"\"\n","dataset = LongerDataset(df, tokenizer, prefix=prefix, max_length=128)\n","\n","val_ratio = 0.85\n","train_dataset, val_dataset = random_split(dataset, [1 - val_ratio, val_ratio])"]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Fine-tuning the model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T15:10:03.605060Z","iopub.status.busy":"2023-09-24T15:10:03.604730Z","iopub.status.idle":"2023-09-24T15:10:04.671893Z","shell.execute_reply":"2023-09-24T15:10:04.670859Z","shell.execute_reply.started":"2023-09-24T15:10:03.605029Z"},"id":"TlqNaB8jIrJW","outputId":"84916cf3-6e6c-47f3-d081-032ec30a4132","trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# create a model for the pretrained model\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from transformers import GenerationConfig\n","\n","gcnf = GenerationConfig(max_length = 128, temperature = 0)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# defining the parameters for training\n","batch_size = 32\n","model_name = model_checkpoint.split(\"/\")[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-longer-2\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=5,\n","    predict_with_generate=True,\n","    fp16=True,\n","    report_to='tensorboard',\n","    generation_config=gcnf\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [[label.strip()] for label in labels]\n","\n","    return preds, labels"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# instead of writing train loop we will use Seq2SeqTrainer\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    # compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"356c8a2de7e84aaf8b7c392448142281","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f61ba451e6d54f029e2cefe1437d2c2a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/705 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.0026724364142864943, 'eval_runtime': 60.2609, 'eval_samples_per_second': 373.891, 'eval_steps_per_second': 11.699, 'epoch': 1.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a8482fe766541cdafa346f0fc6a3a98","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/705 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.001692138030193746, 'eval_runtime': 31.5501, 'eval_samples_per_second': 714.133, 'eval_steps_per_second': 22.345, 'epoch': 2.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba9ba60f6c594ed9922095f4f58f72f9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/705 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.0018509796354919672, 'eval_runtime': 31.942, 'eval_samples_per_second': 705.372, 'eval_steps_per_second': 22.071, 'epoch': 3.0}\n","{'loss': 0.0104, 'learning_rate': 0.0001, 'epoch': 4.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"338eba58ff794b4187b5b0f16f79d4b2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/705 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.0010793991386890411, 'eval_runtime': 33.3511, 'eval_samples_per_second': 675.569, 'eval_steps_per_second': 21.139, 'epoch': 4.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0cb0fd896f0485482a596b8040c6695","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/705 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.0010317499982193112, 'eval_runtime': 39.1672, 'eval_samples_per_second': 575.252, 'eval_steps_per_second': 18.0, 'epoch': 5.0}\n","{'train_runtime': 322.6652, 'train_samples_per_second': 61.627, 'train_steps_per_second': 1.937, 'train_loss': 0.008874515390396118, 'epoch': 5.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=625, training_loss=0.008874515390396118, metrics={'train_runtime': 322.6652, 'train_samples_per_second': 61.627, 'train_steps_per_second': 1.937, 'train_loss': 0.008874515390396118, 'epoch': 5.0})"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# saving model\n","trainer.save_model('longer_best22')"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# loading the model and run inference for it\n","model = AutoModelForSeq2SeqLM.from_pretrained('longer_best22')\n","model.eval()\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def translate(model, inference_request, tokenizer=tokenizer):\n","    input_ids = tokenizer(inference_request, return_tensors=\"pt\").input_ids\n","    outputs = model.generate(input_ids=input_ids)\n","    print(outputs[0].size())\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#val_dataset[15]"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#tokenizer.decode(val_dataset[15]['input_ids'], skip_special_tokens=False, temperature=0)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#tokenizer.decode(val_dataset[15]['labels'][:11], skip_special_tokens=False, temperature=0)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["index = 0"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["17\n"]},{"data":{"text/plain":["'to quote Jake and Oppenheimer: \"I must die, I must feel like a terrible god.\"'"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["index = index + 1\n","print(index)\n","dataset.raw_data[index][0]"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\PMLDL assignment 1 - Text detoxification\\notebooks\\draft.ipynb Cell 32\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#inference_request = dataset.raw_data[index][0]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(inference_request, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m translate(model, inference_request, tokenizer)\n","\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\PMLDL assignment 1 - Text detoxification\\notebooks\\draft.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate\u001b[39m(model, inference_request, tokenizer\u001b[39m=\u001b[39mtokenizer):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(inference_request, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/PMLDL%20assignment%201%20-%20Text%20detoxification/notebooks/draft.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(outputs[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\transformers\\generation\\utils.py:1502\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n\u001b[1;32m-> 1502\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_decoder_input_ids_for_generation(\n\u001b[0;32m   1503\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1504\u001b[0m         model_input_name\u001b[39m=\u001b[39mmodel_input_name,\n\u001b[0;32m   1505\u001b[0m         model_kwargs\u001b[39m=\u001b[39mmodel_kwargs,\n\u001b[0;32m   1506\u001b[0m         decoder_start_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mdecoder_start_token_id,\n\u001b[0;32m   1507\u001b[0m         bos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mbos_token_id,\n\u001b[0;32m   1508\u001b[0m         device\u001b[39m=\u001b[39minputs_tensor\u001b[39m.\u001b[39mdevice,\n\u001b[0;32m   1509\u001b[0m     )\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1511\u001b[0m     input_ids \u001b[39m=\u001b[39m inputs_tensor \u001b[39mif\u001b[39;00m model_input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m model_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\transformers\\generation\\utils.py:685\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_decoder_input_ids_for_generation\u001b[1;34m(self, batch_size, model_input_name, model_kwargs, decoder_start_token_id, bos_token_id, device)\u001b[0m\n\u001b[0;32m    682\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[39m# 2. Encoder-decoder models expect the `decoder_input_ids` to start with a special token. Let's ensure that.\u001b[39;00m\n\u001b[1;32m--> 685\u001b[0m decoder_start_token_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_decoder_start_token_id(decoder_start_token_id, bos_token_id)\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n","File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cuda_transformers_temp\\Lib\\site-packages\\transformers\\generation\\utils.py:722\u001b[0m, in \u001b[0;36mGenerationMixin._get_decoder_start_token_id\u001b[1;34m(self, decoder_start_token_id, bos_token_id)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39melif\u001b[39;00m bos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     \u001b[39mreturn\u001b[39;00m bos_token_id\n\u001b[1;32m--> 722\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    723\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    724\u001b[0m )\n","\u001b[1;31mValueError\u001b[0m: `decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation."]}],"source":["inference_request = prefix + 'Another idea is to join the two columns (a and b) as strings, and check for 12 and 43, i.e.'\n","#inference_request = dataset.raw_data[index][0]\n","input_ids = tokenizer(inference_request, return_tensors=\"pt\").input_ids\n","translate(model, inference_request, tokenizer)"]}],"metadata":{"colab":{"name":"Translation","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
